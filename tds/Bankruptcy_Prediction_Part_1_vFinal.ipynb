{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient (AD), ML and Bankruptcy prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH4FXLQKs-bZ",
    "tags": []
   },
   "source": [
    "***Import packages and definition of useful functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NCWwp9lns-bb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Definition of the Bankrupty prediction use case and data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Altman\n",
    "'s Z-score**\n",
    "\n",
    "Altman (1968) came up with a simple formula known as Z-score, whose expression is:\n",
    "\\begin{equation*}\n",
    "    Z = 0.12 X_1 + 0.14 X_2 + 0.033 X_3 + 0.006 X_4 + 0.999 X_5\n",
    "\\end{equation*}\n",
    "where\n",
    "- $X_1$ is the working capital / total assets\n",
    "- $X_2$ is the retained earnings / total assets\n",
    "- $X_3$ is the earnings before interest and tax / total assets\n",
    "- $X_4$ is the market value of equity / total liabilities\n",
    "- $X_5$ is sale / total assets\n",
    "\n",
    "Based on the value of the score a firm is classified as bankrupt (low score) or healthy (high score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bankruptcy refers to a situation where firm cannot repay its debts to its creditors.\n",
    "\n",
    "As bankruptcy is costly for company stakeholders (employees, supppliers, banks), being able to predict its likelihood is of critical importance.\n",
    "\n",
    "Particularly important nowadays with the rise of the so-called *zombie companies*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7tEI--qs-bc",
    "tags": []
   },
   "source": [
    "We use [the Polish companies bankruptcy dataset](https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data).\n",
    "\n",
    "5 datasets:\n",
    "- For companies in file ``1year``, the reported status is the one observed 5 years later.\n",
    "- For companies in file ``2year``, the reported status is the one observed 4 years later.\n",
    "- ...\n",
    "- For companies in file ``5year``, the reported status is the one observed 1 year later.\n",
    "    \n",
    "Potentially, we could use the 5 datasets and analyze:\n",
    "- whether a firm goes bankrupt or not after some time, irrespective of the number of years.\n",
    "- whether a firm goes bankrupt or not after 1, 2, ..., 5 years. This would be a multiclass classification exercice.\n",
    "    \n",
    "For simplicity, we will focus on a binary classification problem. Working with a single file will also make the computation faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viukmwhts-bc"
   },
   "source": [
    "**Features description of the ``2year``**, there is no header but 64 features (expressed in ratios or in local currency):\n",
    "\n",
    "- X1 net profit / total assets\n",
    "- X2 total liabilities / total assets\n",
    "- X3 working capital / total assets\n",
    "- X4 current assets / short-term liabilities\n",
    "- X5 [(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] * 365\n",
    "- X6 retained earnings / total assets\n",
    "- X7 EBIT / total assets\n",
    "- X8 book value of equity / total liabilities\n",
    "- X9 sales / total assets\n",
    "- X10 equity / total assets\n",
    "- X11 (gross profit + extraordinary items + financial expenses) / total assets\n",
    "- X12 gross profit / short-term liabilities\n",
    "- X13 (gross profit + depreciation) / sales\n",
    "- X14 (gross profit + interest) / total assets\n",
    "- X15 (total liabilities * 365) / (gross profit + depreciation)\n",
    "- X16 (gross profit + depreciation) / total liabilities\n",
    "- X17 total assets / total liabilities\n",
    "- X18 gross profit / total assets\n",
    "- X19 gross profit / sales\n",
    "- X20 (inventory * 365) / sales\n",
    "- X21 sales (n) / sales (n-1)\n",
    "- X22 profit on operating activities / total assets\n",
    "- X23 net profit / sales\n",
    "- X24 gross profit (in 3 years) / total assets\n",
    "- X25 (equity - share capital) / total assets\n",
    "- X26 (net profit + depreciation) / total liabilities\n",
    "- X27 profit on operating activities / financial expenses\n",
    "- X28 working capital / fixed assets\n",
    "- X29 logarithm of total assets\n",
    "- X30 (total liabilities - cash) / sales\n",
    "- X31 (gross profit + interest) / sales\n",
    "- X32 (current liabilities * 365) / cost of products sold\n",
    "- X33 operating expenses / short-term liabilities\n",
    "- X34 operating expenses / total liabilities\n",
    "- X35 profit on sales / total assets\n",
    "- X36 total sales / total assets\n",
    "- X37 (current assets - inventories) / long-term liabilities\n",
    "- X38 constant capital / total assets\n",
    "- X39 profit on sales / sales\n",
    "- X40 (current assets - inventory - receivables) / short-term liabilities\n",
    "- X41 total liabilities / ((profit on operating activities + depreciation) * (12/365))\n",
    "- X42 profit on operating activities / sales\n",
    "- X43 rotation receivables + inventory turnover in days\n",
    "- X44 (receivables * 365) / sales\n",
    "- X45 net profit / inventory\n",
    "- X46 (current assets - inventory) / short-term liabilities\n",
    "- X47 (inventory * 365) / cost of products sold\n",
    "- X48 EBITDA (profit on operating activities - depreciation) / total assets\n",
    "- X49 EBITDA (profit on operating activities - depreciation) / sales\n",
    "- X50 current assets / total liabilities\n",
    "- X51 short-term liabilities / total assets\n",
    "- X52 (short-term liabilities * 365) / cost of products sold)\n",
    "- X53 equity / fixed assets\n",
    "- X54 constant capital / fixed assets\n",
    "- X55 working capital\n",
    "- X56 (sales - cost of products sold) / sales\n",
    "- X57 (current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)\n",
    "- X58 total costs /total sales\n",
    "- X59 long-term liabilities / equity\n",
    "- X60 sales / inventory\n",
    "- X61 sales / receivables\n",
    "- X62 (short-term liabilities *365) / sales\n",
    "- X63 sales / short-term liabilities\n",
    "- X64 sales / fixed assets\n",
    "\n",
    "Finally, the dataset reports for each instance, in the last column, a label taht take on the values 0 and 1:\n",
    "\n",
    "- Feature 65 : 0/1 = survived/went bankrupt in the next year\n",
    "\n",
    "**Our objective is to build a model taht identifies as accurately as possible the true label. This is a supervised learning taxt as the label of each company is reported in the datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl7zGpOqwU9i"
   },
   "source": [
    "**Task 1**: import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fxQb61OPuxMw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10173, 65)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.202350</td>\n",
       "      <td>0.46500</td>\n",
       "      <td>0.240380</td>\n",
       "      <td>1.5171</td>\n",
       "      <td>-14.547</td>\n",
       "      <td>0.510690</td>\n",
       "      <td>0.25366</td>\n",
       "      <td>0.91816</td>\n",
       "      <td>1.15190</td>\n",
       "      <td>0.42695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13184</td>\n",
       "      <td>0.473950</td>\n",
       "      <td>0.86816</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>8.5487</td>\n",
       "      <td>5.16550</td>\n",
       "      <td>107.740</td>\n",
       "      <td>3.38790</td>\n",
       "      <td>5.3440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030073</td>\n",
       "      <td>0.59563</td>\n",
       "      <td>0.186680</td>\n",
       "      <td>1.3382</td>\n",
       "      <td>-37.859</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>0.04167</td>\n",
       "      <td>0.67890</td>\n",
       "      <td>0.32356</td>\n",
       "      <td>0.40437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12146</td>\n",
       "      <td>0.074369</td>\n",
       "      <td>0.87235</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.5264</td>\n",
       "      <td>0.63305</td>\n",
       "      <td>622.660</td>\n",
       "      <td>0.58619</td>\n",
       "      <td>1.2381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.257860</td>\n",
       "      <td>0.29949</td>\n",
       "      <td>0.665190</td>\n",
       "      <td>3.2211</td>\n",
       "      <td>71.799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31877</td>\n",
       "      <td>2.33200</td>\n",
       "      <td>1.67620</td>\n",
       "      <td>0.69841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16499</td>\n",
       "      <td>0.369210</td>\n",
       "      <td>0.81614</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.3325</td>\n",
       "      <td>3.19850</td>\n",
       "      <td>65.215</td>\n",
       "      <td>5.59690</td>\n",
       "      <td>47.4660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.227160</td>\n",
       "      <td>0.67850</td>\n",
       "      <td>0.042784</td>\n",
       "      <td>1.0828</td>\n",
       "      <td>-88.212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.28505</td>\n",
       "      <td>0.47384</td>\n",
       "      <td>1.32410</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29358</td>\n",
       "      <td>0.706570</td>\n",
       "      <td>0.78617</td>\n",
       "      <td>0.48456</td>\n",
       "      <td>5.2309</td>\n",
       "      <td>5.06750</td>\n",
       "      <td>142.460</td>\n",
       "      <td>2.56210</td>\n",
       "      <td>3.0066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.085443</td>\n",
       "      <td>0.38039</td>\n",
       "      <td>0.359230</td>\n",
       "      <td>1.9444</td>\n",
       "      <td>21.731</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.10823</td>\n",
       "      <td>1.37140</td>\n",
       "      <td>1.11260</td>\n",
       "      <td>0.52167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10124</td>\n",
       "      <td>0.163790</td>\n",
       "      <td>0.89876</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.7035</td>\n",
       "      <td>4.00200</td>\n",
       "      <td>89.058</td>\n",
       "      <td>4.09840</td>\n",
       "      <td>5.9874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1         2       3       4         5        6        7   \\\n",
       "0  0.202350  0.46500  0.240380  1.5171 -14.547  0.510690  0.25366  0.91816   \n",
       "1  0.030073  0.59563  0.186680  1.3382 -37.859 -0.000319  0.04167  0.67890   \n",
       "2  0.257860  0.29949  0.665190  3.2211  71.799  0.000000  0.31877  2.33200   \n",
       "3  0.227160  0.67850  0.042784  1.0828 -88.212  0.000000  0.28505  0.47384   \n",
       "4  0.085443  0.38039  0.359230  1.9444  21.731  0.187900  0.10823  1.37140   \n",
       "\n",
       "        8        9   ...       55        56       57       58      59  \\\n",
       "0  1.15190  0.42695  ...  0.13184  0.473950  0.86816  0.00024  8.5487   \n",
       "1  0.32356  0.40437  ...  0.12146  0.074369  0.87235  0.00000  1.5264   \n",
       "2  1.67620  0.69841  ...  0.16499  0.369210  0.81614  0.00000  4.3325   \n",
       "3  1.32410  0.32150  ...  0.29358  0.706570  0.78617  0.48456  5.2309   \n",
       "4  1.11260  0.52167  ...  0.10124  0.163790  0.89876  0.00000  5.7035   \n",
       "\n",
       "        60       61       62       63  64  \n",
       "0  5.16550  107.740  3.38790   5.3440   0  \n",
       "1  0.63305  622.660  0.58619   1.2381   0  \n",
       "2  3.19850   65.215  5.59690  47.4660   0  \n",
       "3  5.06750  142.460  2.56210   3.0066   0  \n",
       "4  4.00200   89.058  4.09840   5.9874   0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"year2.txt\", header=None, na_values=[\"?\"])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0pZYygjs-be",
    "tags": []
   },
   "source": [
    "### 1.2 Missing data\n",
    "\n",
    "Missing values are reported using ? character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw9-7o08s-be"
   },
   "source": [
    "**Task 2**: Generate a barplot that displays for each feature the number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0KAzhIjVs-bf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# insert your code here\n",
    "plt.figure(figsize=(16,4))\n",
    "(len(df.index)-df.count()).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible solution to handle missing values is to inpute a replacement value (e.g. mean or median of the feature)\n",
    "\n",
    "Here, we use a simpler solution: we remove all features that contains more than 50 missing values. Once this is done, we remove all instance that still contains missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BHpszucs-bf"
   },
   "source": [
    "**Task 3**:\n",
    "- Step 1: Remove from the dataset all features that contain more than 50 missing values\n",
    "- Step 2: Remove all missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5GnW-4_ns-bf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10088, 38)\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "df = df.drop(df.columns[df.apply(lambda col: col.isnull().sum() > 50)], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "Training a ML model amounts to finding the model parameter values that allow to generate the predictions that best match the actual mabels\n",
    "\n",
    "if the model is trained using the whole dataset, we will get the optimal parameter values for the dataste, which will allow us to achieve the best in-sample predictions.\n",
    "\n",
    "Yetn there is no guarantee on how general the model is and how it will perform out-of-sample, that is on a new mdataste, using data the model has never seen before.\n",
    "\n",
    "For that reason, in every ML project, the original dataset is always split into 2 sets (at least):\n",
    "\n",
    "- A train set: the dataset on which training is performed and models are fine-tuned\n",
    "- A test set: the dataset containing unseen data, which is used to assess the performance of the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a1Ob426s-bf",
    "tags": []
   },
   "source": [
    "### 1.4 Differences in feature value magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "ML algorithms do not perform weel in general when features greatly differ in magnitude or are expressed using different scales (here ratios and currency units)\n",
    "\n",
    "Two standard procedure allow to transform feature so that they get all expressed in the same scale:\n",
    "- Min-max scaling or normalization\n",
    "- Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "A7N_VkBJs-bg",
    "outputId": "86215be4-a3a4-43db-80f4-fffc5b10ca33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>54</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>62</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>1.008800e+04</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "      <td>10088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.056863</td>\n",
       "      <td>0.633485</td>\n",
       "      <td>0.080845</td>\n",
       "      <td>3.455431</td>\n",
       "      <td>-120.644258</td>\n",
       "      <td>-0.103082</td>\n",
       "      <td>0.136543</td>\n",
       "      <td>3.816087</td>\n",
       "      <td>2.849920</td>\n",
       "      <td>0.349746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082498</td>\n",
       "      <td>2.772320</td>\n",
       "      <td>0.516627</td>\n",
       "      <td>5.970039e+03</td>\n",
       "      <td>-0.039787</td>\n",
       "      <td>12.565388</td>\n",
       "      <td>3.056922</td>\n",
       "      <td>16.875062</td>\n",
       "      <td>11.188472</td>\n",
       "      <td>0.039155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.679260</td>\n",
       "      <td>6.546280</td>\n",
       "      <td>6.540730</td>\n",
       "      <td>15.705346</td>\n",
       "      <td>6879.776696</td>\n",
       "      <td>6.561223</td>\n",
       "      <td>6.499309</td>\n",
       "      <td>29.361301</td>\n",
       "      <td>97.032686</td>\n",
       "      <td>6.545727</td>\n",
       "      <td>...</td>\n",
       "      <td>6.222518</td>\n",
       "      <td>13.564894</td>\n",
       "      <td>6.545366</td>\n",
       "      <td>6.571290e+04</td>\n",
       "      <td>12.293023</td>\n",
       "      <td>634.211780</td>\n",
       "      <td>239.163399</td>\n",
       "      <td>296.688387</td>\n",
       "      <td>234.530106</td>\n",
       "      <td>0.193974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-61.628000</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>-479.960000</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>-438250.000000</td>\n",
       "      <td>-508.410000</td>\n",
       "      <td>-61.628000</td>\n",
       "      <td>-1.594500</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-479.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.679200</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-1.805200e+06</td>\n",
       "      <td>-979.250000</td>\n",
       "      <td>-4.549700</td>\n",
       "      <td>-189.580000</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>-0.367890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.279883</td>\n",
       "      <td>0.011708</td>\n",
       "      <td>1.028650</td>\n",
       "      <td>-50.043250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.393083</td>\n",
       "      <td>1.028000</td>\n",
       "      <td>0.276637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039410</td>\n",
       "      <td>0.751385</td>\n",
       "      <td>0.196350</td>\n",
       "      <td>5.024375e+00</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.872880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.724150</td>\n",
       "      <td>3.120300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.050127</td>\n",
       "      <td>0.489650</td>\n",
       "      <td>0.188745</td>\n",
       "      <td>1.521550</td>\n",
       "      <td>-2.091050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060594</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>1.290600</td>\n",
       "      <td>0.489460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023420</td>\n",
       "      <td>1.189850</td>\n",
       "      <td>0.353430</td>\n",
       "      <td>8.389200e+02</td>\n",
       "      <td>0.127470</td>\n",
       "      <td>0.950005</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>7.013950</td>\n",
       "      <td>5.218200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.705630</td>\n",
       "      <td>0.398108</td>\n",
       "      <td>2.727050</td>\n",
       "      <td>50.972750</td>\n",
       "      <td>0.075571</td>\n",
       "      <td>0.166465</td>\n",
       "      <td>2.506250</td>\n",
       "      <td>2.263450</td>\n",
       "      <td>0.700402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123122</td>\n",
       "      <td>2.146875</td>\n",
       "      <td>0.550398</td>\n",
       "      <td>4.025350e+03</td>\n",
       "      <td>0.310262</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.250970</td>\n",
       "      <td>11.108750</td>\n",
       "      <td>9.023075</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.372700</td>\n",
       "      <td>480.960000</td>\n",
       "      <td>0.996880</td>\n",
       "      <td>782.250000</td>\n",
       "      <td>70686.000000</td>\n",
       "      <td>14.756000</td>\n",
       "      <td>649.230000</td>\n",
       "      <td>1597.400000</td>\n",
       "      <td>9742.300000</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>...</td>\n",
       "      <td>623.850000</td>\n",
       "      <td>544.560000</td>\n",
       "      <td>480.960000</td>\n",
       "      <td>3.657400e+06</td>\n",
       "      <td>147.190000</td>\n",
       "      <td>59672.000000</td>\n",
       "      <td>23853.000000</td>\n",
       "      <td>26862.000000</td>\n",
       "      <td>23454.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3              4   \\\n",
       "count  10088.000000  10088.000000  10088.000000  10088.000000   10088.000000   \n",
       "mean       0.056863      0.633485      0.080845      3.455431    -120.644258   \n",
       "std        0.679260      6.546280      6.540730     15.705346    6879.776696   \n",
       "min      -61.628000      0.000626   -479.960000      0.002079 -438250.000000   \n",
       "25%        0.000646      0.279883      0.011708      1.028650     -50.043250   \n",
       "50%        0.050127      0.489650      0.188745      1.521550      -2.091050   \n",
       "75%        0.141700      0.705630      0.398108      2.727050      50.972750   \n",
       "max        7.372700    480.960000      0.996880    782.250000   70686.000000   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  10088.000000  10088.000000  10088.000000  10088.000000  10088.000000   \n",
       "mean      -0.103082      0.136543      3.816087      2.849920      0.349746   \n",
       "std        6.561223      6.499309     29.361301     97.032686      6.545727   \n",
       "min     -508.410000    -61.628000     -1.594500     -0.000857   -479.910000   \n",
       "25%        0.000000      0.002411      0.393083      1.028000      0.276637   \n",
       "50%        0.000000      0.060594      0.999350      1.290600      0.489460   \n",
       "75%        0.075571      0.166465      2.506250      2.263450      0.700402   \n",
       "max       14.756000    649.230000   1597.400000   9742.300000      0.999370   \n",
       "\n",
       "       ...            47            49            50            54  \\\n",
       "count  ...  10088.000000  10088.000000  10088.000000  1.008800e+04   \n",
       "mean   ...      0.082498      2.772320      0.516627  5.970039e+03   \n",
       "std    ...      6.222518     13.564894      6.545366  6.571290e+04   \n",
       "min    ...     -9.679200      0.002079      0.000516 -1.805200e+06   \n",
       "25%    ...     -0.039410      0.751385      0.196350  5.024375e+00   \n",
       "50%    ...      0.023420      1.189850      0.353430  8.389200e+02   \n",
       "75%    ...      0.123122      2.146875      0.550398  4.025350e+03   \n",
       "max    ...    623.850000    544.560000    480.960000  3.657400e+06   \n",
       "\n",
       "                 56            57            58            60            62  \\\n",
       "count  10088.000000  10088.000000  10088.000000  10088.000000  10088.000000   \n",
       "mean      -0.039787     12.565388      3.056922     16.875062     11.188472   \n",
       "std       12.293023    634.211780    239.163399    296.688387    234.530106   \n",
       "min     -979.250000     -4.549700   -189.580000     -0.007521     -0.367890   \n",
       "25%        0.010760      0.872880      0.000000      4.724150      3.120300   \n",
       "50%        0.127470      0.950005      0.005363      7.013950      5.218200   \n",
       "75%        0.310262      0.993390      0.250970     11.108750      9.023075   \n",
       "max      147.190000  59672.000000  23853.000000  26862.000000  23454.000000   \n",
       "\n",
       "                 64  \n",
       "count  10088.000000  \n",
       "mean       0.039155  \n",
       "std        0.193974  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scaling is performed, the scaling parameters must be computed using the train set only to avoid any look-ahead bias.\n",
    "\n",
    "Scaling is thus performed in three steps:\n",
    "\n",
    "    - Step 1: compute the min and max of each feature in the train set\n",
    "    - Step 2: perform feature scaling on the train set\n",
    "    - Step 3: perform feature scaling on the test set using values found at step 1\n",
    "    \n",
    "Note that scaling is applied to features only, not labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D0tFvmhgs-bg"
   },
   "outputs": [],
   "source": [
    "def normalize(*arg): # df [,min_vals,max_vals]\n",
    "    \"\"\" Normalizes the features of a dataframe\n",
    "    Up to 3 arguments\n",
    "    - 1st argument (mandatory) = df: name of dataframe to normalize\n",
    "    - 2nd argument (optional): min_vals = list of min value of each feature\n",
    "    - 3rd argument (optional): max_vals = list of max value of each feature\n",
    "    If df only is passed, the function normalizes X_ij, i.e. ith value of feature j as:\n",
    "                               (X_ij - min_j)/(max_j - min_j)                           (1)\n",
    "    and returns the min_j and max_j lists\n",
    "    If df, min_vals and max_vals are passed, df is normalized as per (1) using these values\n",
    "    \"\"\"\n",
    "    df = arg[0]\n",
    "    result = df.copy()\n",
    "    if len(arg)==1:\n",
    "        minval, maxval = [], []\n",
    "        for feature_name in df.columns:\n",
    "            min_value = df[feature_name].min()\n",
    "            max_value = df[feature_name].max()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "            minval.append(min_value)\n",
    "            maxval.append(max_value)\n",
    "        return result, minval, maxval\n",
    "    else:\n",
    "        minvals = arg[1]\n",
    "        maxvals = arg[2]\n",
    "        for i in range(df.shape[1]):\n",
    "            result.iloc[:,i] = (df.iloc[:,i] - minvals[i]) / (maxvals[i] - minvals[i])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "###  1.5 Balancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "The issue (for ML) is that the number of surviving firms is 24 times as large as the number of dead firms: the dataset is unbalanced\n",
    "\n",
    "If we train a ML model on an unbalanced dataset the resulting parameters will be biased towards predicting the majority class\n",
    "\n",
    "In the extreme case where a naive model returns the prediction 0 for all instances, the predictions will be correct 96\\% of the time. Yet, its out-of-sample ability to identify future bankrupt firms will be very small.\n",
    "\n",
    "To make a model identifying the features that are useful at predicting bankruptcy, trainig must be performed on a balanced dataste\n",
    "\n",
    "We will build a train daatset that has the same proportion of future surviving and bankrupt firms\n",
    "\n",
    "Dataset balancing can be performed in two ways:\n",
    "- Undersampling: randomly remove surviving firms on order to get the same 50\\% proportion of survivig and bankrupt firms\n",
    "- Oversampling:\n",
    "    - Naive: randomly duplicate examples in the minority class\n",
    "    - SMOTE: Synthetic Minority Oversampling TEchnique. Generate artificial examples based on k-nearest neighbours from instances in the minority class.\n",
    "\n",
    "It is possible to combine under- and over- sampling.\n",
    "\n",
    "We will build a train set that contains 75\\% of the dead firms from the original dataset and the same number of surviving firms. Both the dead firms and the surviving firms will be randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3thS-Vgx0Zf"
   },
   "source": [
    "**Task 4-1**: Plot a histogram that reports the number of surviving firms (label=0) and dead firms (label=1) in the **dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SVj9dMCBs-bh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9693.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         395.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# insert your code here\n",
    "plt.hist(df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPVfnnREzgf5"
   },
   "source": [
    "**Task 4-2**:\n",
    "- Generate a dataframe named `surv` that contains surviving firms, and a dataframe named `dead` that contains dead firms\n",
    "- Store in variables `l_surv`and `l_dead`the number of surviving firms and dead firms, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NimW7NLhs-bh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9693 395\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "surv, dead = df[df.iloc[:, -1]==0], df[df.iloc[:, -1]==1]\n",
    "l_surv, l_dead = len(surv), len(dead)\n",
    "print(l_surv, l_dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75ZUHjNi0aPy"
   },
   "source": [
    "**Task 5**:\n",
    "- Using the `sample` function, create a `dead_train` dataframe that contains 75% of the dead firms, where those firms are ramdomly selected\n",
    "- Create a `surv_train` dataframe that contains as many randomly selected surviving firms as the number of randomly selected dead firms\n",
    "- Combine vertically the `dead_train` and the `surv_train` dataframes into a single dataframe names `Train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KlsLFIfzs-bh"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # for reproductibility of results\n",
    "\n",
    "# insert your code here\n",
    "dead_train = dead.sample(int(l_dead*.75))\n",
    "surv_train = surv.sample(int(l_dead*.75))\n",
    "\n",
    "Train = pd.concat([dead_train, surv_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 296\n"
     ]
    }
   ],
   "source": [
    "l_surv_train, l_dead_train = len(surv_train), len(dead_train)\n",
    "print(l_surv_train, l_dead_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1K-8jX1s-bh"
   },
   "source": [
    "All firms not selected in the Train dataframe must now be put in the Test dataframe.\n",
    "\n",
    "**Task 6**: \n",
    "- Create a `dead_remain` dataframe with all instances of dead firms **<u>not</u>** in the `dead_train` dataframe\n",
    "- Create a `surv_remain` dataframe with all instances of surviving firms **<u>not</u>** in the `surv_train` dataframe\n",
    "- Combine vertically the `dead_remain` and the `surv_remain` dataframes into a single dataframe named `Test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-If3Fnp1s-bi"
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "dead_remain = dead[~dead.isin(dead_train)].dropna() # observations in dead_train are reported as NaN\n",
    "surv_remain = surv[~surv.isin(surv_train)].dropna()\n",
    "\n",
    "Test = pd.concat([dead_remain, surv_remain], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9397 99\n"
     ]
    }
   ],
   "source": [
    "l_surv_remain, l_dead_remain = len(surv_remain), len(dead_remain)\n",
    "print(l_surv_remain, l_dead_remain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ1WbTsOs-bi"
   },
   "source": [
    "The next step is to separate the labels from the features\n",
    "\n",
    "**Task 7**: \n",
    "- Create the `Ytrain` and `Ytest` dataframes with the <u>labels</u> from the `Train` and the `Test` dataframes, respectively\n",
    "- Create the `Xtrain` and `Xtest` dataframes with the <u>features</u> from the `Train` and the `Test` dataframes, respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Xmu8m-RRs-bi"
   },
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "# insert your code here\n",
    "Ytrain = Train.iloc[:,-1]\n",
    "Ytest = Test.iloc[:,-1]\n",
    "\n",
    "# Extract features\n",
    "# insert your code here\n",
    "Xtrain = Train.iloc[:,:-1] \n",
    "Xtest = Test.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBrd_LV4s-bi"
   },
   "source": [
    "Finally, we can normalize both the train and the test sets.\n",
    "\n",
    "**Task 8**:\n",
    "- Create the `Xtrain_norm` dataframe by applying feature scaling to the `Xtrain` dataframe\n",
    "- Create the `Xtest_norm` dataframe by applying feature scaling to the `Xtest` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TDHX_3tss-bi"
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "Xtrain_norm, min_vals, max_vals = normalize(Xtrain)\n",
    "Xtest_norm = normalize(Xtest, min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HFggko5gs-bj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>47</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>54</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.013282</td>\n",
       "      <td>1.440260</td>\n",
       "      <td>-0.723301</td>\n",
       "      <td>3.722466</td>\n",
       "      <td>-213.398384</td>\n",
       "      <td>-1.005629</td>\n",
       "      <td>0.025442</td>\n",
       "      <td>4.032041</td>\n",
       "      <td>1.872854</td>\n",
       "      <td>-0.459846</td>\n",
       "      <td>...</td>\n",
       "      <td>2.882442</td>\n",
       "      <td>-0.008606</td>\n",
       "      <td>3.211994</td>\n",
       "      <td>1.310033</td>\n",
       "      <td>2687.924067</td>\n",
       "      <td>-0.105317</td>\n",
       "      <td>1.839165</td>\n",
       "      <td>40.811098</td>\n",
       "      <td>17.193168</td>\n",
       "      <td>7.376815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.534558</td>\n",
       "      <td>19.747443</td>\n",
       "      <td>19.735366</td>\n",
       "      <td>24.979245</td>\n",
       "      <td>6104.534815</td>\n",
       "      <td>20.981582</td>\n",
       "      <td>0.539040</td>\n",
       "      <td>34.821886</td>\n",
       "      <td>1.804231</td>\n",
       "      <td>19.744712</td>\n",
       "      <td>...</td>\n",
       "      <td>22.767715</td>\n",
       "      <td>0.448191</td>\n",
       "      <td>24.884444</td>\n",
       "      <td>19.752124</td>\n",
       "      <td>27801.568825</td>\n",
       "      <td>5.115328</td>\n",
       "      <td>20.218933</td>\n",
       "      <td>980.379744</td>\n",
       "      <td>75.118744</td>\n",
       "      <td>17.539184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.815000</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-479.960000</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>-146150.000000</td>\n",
       "      <td>-508.410000</td>\n",
       "      <td>-6.815000</td>\n",
       "      <td>-0.997810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-479.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>-7.881700</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-259740.000000</td>\n",
       "      <td>-76.982000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-80.147000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.018667</td>\n",
       "      <td>0.345823</td>\n",
       "      <td>-0.067543</td>\n",
       "      <td>0.871020</td>\n",
       "      <td>-64.328500</td>\n",
       "      <td>-0.007887</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>0.242565</td>\n",
       "      <td>1.013975</td>\n",
       "      <td>0.195213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440908</td>\n",
       "      <td>-0.054973</td>\n",
       "      <td>0.661705</td>\n",
       "      <td>0.255537</td>\n",
       "      <td>-268.292500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.685025</td>\n",
       "      <td>2.714975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.027529</td>\n",
       "      <td>0.586305</td>\n",
       "      <td>0.114150</td>\n",
       "      <td>1.257850</td>\n",
       "      <td>-18.437000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035270</td>\n",
       "      <td>0.683745</td>\n",
       "      <td>1.214700</td>\n",
       "      <td>0.400245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739520</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>1.003650</td>\n",
       "      <td>0.421305</td>\n",
       "      <td>444.820000</td>\n",
       "      <td>0.087228</td>\n",
       "      <td>0.966220</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>7.241400</td>\n",
       "      <td>4.347150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.112798</td>\n",
       "      <td>0.793290</td>\n",
       "      <td>0.293238</td>\n",
       "      <td>2.161025</td>\n",
       "      <td>28.649250</td>\n",
       "      <td>0.014939</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>1.822100</td>\n",
       "      <td>2.328850</td>\n",
       "      <td>0.628208</td>\n",
       "      <td>...</td>\n",
       "      <td>1.405500</td>\n",
       "      <td>0.103478</td>\n",
       "      <td>1.613200</td>\n",
       "      <td>0.647305</td>\n",
       "      <td>2269.950000</td>\n",
       "      <td>0.286965</td>\n",
       "      <td>1.000775</td>\n",
       "      <td>0.323578</td>\n",
       "      <td>11.382500</td>\n",
       "      <td>7.807550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.372700</td>\n",
       "      <td>480.960000</td>\n",
       "      <td>0.975730</td>\n",
       "      <td>431.120000</td>\n",
       "      <td>22876.000000</td>\n",
       "      <td>0.772020</td>\n",
       "      <td>7.372700</td>\n",
       "      <td>664.010000</td>\n",
       "      <td>22.743000</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>...</td>\n",
       "      <td>421.770000</td>\n",
       "      <td>0.928760</td>\n",
       "      <td>431.120000</td>\n",
       "      <td>480.960000</td>\n",
       "      <td>537580.000000</td>\n",
       "      <td>38.355000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>23853.000000</td>\n",
       "      <td>1324.500000</td>\n",
       "      <td>376.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3              4   \\\n",
       "count  592.000000  592.000000  592.000000  592.000000     592.000000   \n",
       "mean     0.013282    1.440260   -0.723301    3.722466    -213.398384   \n",
       "std      0.534558   19.747443   19.735366   24.979245    6104.534815   \n",
       "min     -6.815000    0.001504 -479.960000    0.002079 -146150.000000   \n",
       "25%     -0.018667    0.345823   -0.067543    0.871020     -64.328500   \n",
       "50%      0.027529    0.586305    0.114150    1.257850     -18.437000   \n",
       "75%      0.112798    0.793290    0.293238    2.161025      28.649250   \n",
       "max      7.372700  480.960000    0.975730  431.120000   22876.000000   \n",
       "\n",
       "               5           6           7           8           9   ...  \\\n",
       "count  592.000000  592.000000  592.000000  592.000000  592.000000  ...   \n",
       "mean    -1.005629    0.025442    4.032041    1.872854   -0.459846  ...   \n",
       "std     20.981582    0.539040   34.821886    1.804231   19.744712  ...   \n",
       "min   -508.410000   -6.815000   -0.997810    0.000000 -479.910000  ...   \n",
       "25%     -0.007887   -0.018488    0.242565    1.013975    0.195213  ...   \n",
       "50%      0.000000    0.035270    0.683745    1.214700    0.400245  ...   \n",
       "75%      0.014939    0.128980    1.822100    2.328850    0.628208  ...   \n",
       "max      0.772020    7.372700  664.010000   22.743000    0.998500  ...   \n",
       "\n",
       "               45          47          49          50             54  \\\n",
       "count  592.000000  592.000000  592.000000  592.000000     592.000000   \n",
       "mean     2.882442   -0.008606    3.211994    1.310033    2687.924067   \n",
       "std     22.767715    0.448191   24.884444   19.752124   27801.568825   \n",
       "min      0.000180   -7.881700    0.002079    0.001504 -259740.000000   \n",
       "25%      0.440908   -0.054973    0.661705    0.255537    -268.292500   \n",
       "50%      0.739520    0.010124    1.003650    0.421305     444.820000   \n",
       "75%      1.405500    0.103478    1.613200    0.647305    2269.950000   \n",
       "max    421.770000    0.928760  431.120000  480.960000  537580.000000   \n",
       "\n",
       "               56          57            58           60          62  \n",
       "count  592.000000  592.000000    592.000000   592.000000  592.000000  \n",
       "mean    -0.105317    1.839165     40.811098    17.193168    7.376815  \n",
       "std      5.115328   20.218933    980.379744    75.118744   17.539184  \n",
       "min    -76.982000    0.000000    -80.147000     0.000000    0.000000  \n",
       "25%      0.000000    0.889915      0.000000     4.685025    2.714975  \n",
       "50%      0.087228    0.966220      0.005827     7.241400    4.347150  \n",
       "75%      0.286965    1.000775      0.323578    11.382500    7.807550  \n",
       "max     38.355000  492.000000  23853.000000  1324.500000  376.040000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V9205N1-s-bj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>47</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>54</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>592.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.481282</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.996467</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.863397</td>\n",
       "      <td>0.996509</td>\n",
       "      <td>0.482139</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.082349</td>\n",
       "      <td>0.996968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.893608</td>\n",
       "      <td>0.007446</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.329138</td>\n",
       "      <td>0.666540</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.019617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.037678</td>\n",
       "      <td>0.041059</td>\n",
       "      <td>0.041035</td>\n",
       "      <td>0.057941</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.041206</td>\n",
       "      <td>0.037993</td>\n",
       "      <td>0.052363</td>\n",
       "      <td>0.079331</td>\n",
       "      <td>0.041057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053981</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>0.057721</td>\n",
       "      <td>0.041068</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.044351</td>\n",
       "      <td>0.041095</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.046642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.479030</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.864279</td>\n",
       "      <td>0.998468</td>\n",
       "      <td>0.479043</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.044584</td>\n",
       "      <td>0.998330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.888345</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.325430</td>\n",
       "      <td>0.667453</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.007220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.482286</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.998209</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.864551</td>\n",
       "      <td>0.998484</td>\n",
       "      <td>0.482832</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.053410</td>\n",
       "      <td>0.998756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.895733</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.326324</td>\n",
       "      <td>0.668209</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.011560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.488296</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.998581</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.864829</td>\n",
       "      <td>0.998513</td>\n",
       "      <td>0.489437</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.999230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.906329</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.328613</td>\n",
       "      <td>0.669941</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.020763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  592.000000  592.000000  592.000000  592.000000  592.000000  592.000000   \n",
       "mean     0.481282    0.002991    0.996467    0.008630    0.863397    0.996509   \n",
       "std      0.037678    0.041059    0.041035    0.057941    0.036116    0.041206   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.479030    0.000716    0.997831    0.002016    0.864279    0.998468   \n",
       "50%      0.482286    0.001216    0.998209    0.002913    0.864551    0.998484   \n",
       "75%      0.488296    0.001646    0.998581    0.005008    0.864829    0.998513   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               6           7           8           9   ...          45  \\\n",
       "count  592.000000  592.000000  592.000000  592.000000  ...  592.000000   \n",
       "mean     0.482139    0.007564    0.082349    0.996968  ...    0.006834   \n",
       "std      0.037993    0.052363    0.079331    0.041057  ...    0.053981   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.479043    0.001865    0.044584    0.998330  ...    0.001045   \n",
       "50%      0.482832    0.002529    0.053410    0.998756  ...    0.001753   \n",
       "75%      0.489437    0.004240    0.102399    0.999230  ...    0.003332   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "               47          49          50          54          56          57  \\\n",
       "count  592.000000  592.000000  592.000000  592.000000  592.000000  592.000000   \n",
       "mean     0.893608    0.007446    0.002721    0.329138    0.666540    0.003738   \n",
       "std      0.050870    0.057721    0.041068    0.034869    0.044351    0.041095   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.888345    0.001530    0.000528    0.325430    0.667453    0.001809   \n",
       "50%      0.895733    0.002323    0.000873    0.326324    0.668209    0.001964   \n",
       "75%      0.906329    0.003737    0.001343    0.328613    0.669941    0.002034   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               58          60          62  \n",
       "count  592.000000  592.000000  592.000000  \n",
       "mean     0.005054    0.012981    0.019617  \n",
       "std      0.040963    0.056715    0.046642  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.003349    0.003537    0.007220  \n",
       "50%      0.003349    0.005467    0.011560  \n",
       "75%      0.003362    0.008594    0.020763  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DdeuRUn8s-bj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>47</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>54</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9.496000e+03</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.059580</td>\n",
       "      <td>0.583189</td>\n",
       "      <td>0.130977</td>\n",
       "      <td>3.438784</td>\n",
       "      <td>-114.861777</td>\n",
       "      <td>-0.046815</td>\n",
       "      <td>0.143469</td>\n",
       "      <td>3.802624</td>\n",
       "      <td>2.910833</td>\n",
       "      <td>0.400218</td>\n",
       "      <td>...</td>\n",
       "      <td>2.586799</td>\n",
       "      <td>0.088178</td>\n",
       "      <td>2.744910</td>\n",
       "      <td>0.467164</td>\n",
       "      <td>6.174652e+03</td>\n",
       "      <td>-0.035702</td>\n",
       "      <td>13.234083</td>\n",
       "      <td>0.703249</td>\n",
       "      <td>16.855230</td>\n",
       "      <td>11.426099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.687204</td>\n",
       "      <td>4.605438</td>\n",
       "      <td>4.600310</td>\n",
       "      <td>14.939694</td>\n",
       "      <td>6925.480649</td>\n",
       "      <td>4.275350</td>\n",
       "      <td>6.697446</td>\n",
       "      <td>28.988943</td>\n",
       "      <td>100.010548</td>\n",
       "      <td>4.605299</td>\n",
       "      <td>...</td>\n",
       "      <td>14.118622</td>\n",
       "      <td>6.412551</td>\n",
       "      <td>12.526885</td>\n",
       "      <td>4.602961</td>\n",
       "      <td>6.736910e+04</td>\n",
       "      <td>12.606009</td>\n",
       "      <td>653.658670</td>\n",
       "      <td>29.089834</td>\n",
       "      <td>305.222767</td>\n",
       "      <td>241.689277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-61.628000</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>-440.740000</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>-438250.000000</td>\n",
       "      <td>-398.120000</td>\n",
       "      <td>-61.628000</td>\n",
       "      <td>-1.594500</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-440.740000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.256100</td>\n",
       "      <td>-9.679200</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-1.805200e+06</td>\n",
       "      <td>-979.250000</td>\n",
       "      <td>-4.549700</td>\n",
       "      <td>-189.580000</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>-0.367890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.277290</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>1.039300</td>\n",
       "      <td>-49.059250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.409535</td>\n",
       "      <td>1.029000</td>\n",
       "      <td>0.283410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592220</td>\n",
       "      <td>-0.038386</td>\n",
       "      <td>0.759858</td>\n",
       "      <td>0.194315</td>\n",
       "      <td>1.466975e+01</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>0.871975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.725850</td>\n",
       "      <td>3.149050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.051782</td>\n",
       "      <td>0.483060</td>\n",
       "      <td>0.193560</td>\n",
       "      <td>1.541800</td>\n",
       "      <td>-0.939565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062915</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>1.295700</td>\n",
       "      <td>0.495375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015050</td>\n",
       "      <td>0.024399</td>\n",
       "      <td>1.206650</td>\n",
       "      <td>0.348455</td>\n",
       "      <td>8.793950e+02</td>\n",
       "      <td>0.129680</td>\n",
       "      <td>0.948945</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>7.005200</td>\n",
       "      <td>5.273050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.143402</td>\n",
       "      <td>0.698887</td>\n",
       "      <td>0.401905</td>\n",
       "      <td>2.766825</td>\n",
       "      <td>51.780500</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>0.168065</td>\n",
       "      <td>2.542125</td>\n",
       "      <td>2.262225</td>\n",
       "      <td>0.704827</td>\n",
       "      <td>...</td>\n",
       "      <td>1.896000</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>2.172925</td>\n",
       "      <td>0.544880</td>\n",
       "      <td>4.181975e+03</td>\n",
       "      <td>0.312032</td>\n",
       "      <td>0.993080</td>\n",
       "      <td>0.247670</td>\n",
       "      <td>11.079250</td>\n",
       "      <td>9.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.138400</td>\n",
       "      <td>441.740000</td>\n",
       "      <td>0.996880</td>\n",
       "      <td>782.250000</td>\n",
       "      <td>70686.000000</td>\n",
       "      <td>14.756000</td>\n",
       "      <td>649.230000</td>\n",
       "      <td>1597.400000</td>\n",
       "      <td>9742.300000</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>...</td>\n",
       "      <td>782.250000</td>\n",
       "      <td>623.850000</td>\n",
       "      <td>544.560000</td>\n",
       "      <td>441.740000</td>\n",
       "      <td>3.657400e+06</td>\n",
       "      <td>147.190000</td>\n",
       "      <td>59672.000000</td>\n",
       "      <td>2517.700000</td>\n",
       "      <td>26862.000000</td>\n",
       "      <td>23454.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3              4   \\\n",
       "count  9496.000000  9496.000000  9496.000000  9496.000000    9496.000000   \n",
       "mean      0.059580     0.583189     0.130977     3.438784    -114.861777   \n",
       "std       0.687204     4.605438     4.600310    14.939694    6925.480649   \n",
       "min     -61.628000     0.000626  -440.740000     0.002264 -438250.000000   \n",
       "25%       0.000894     0.277290     0.015974     1.039300     -49.059250   \n",
       "50%       0.051782     0.483060     0.193560     1.541800      -0.939565   \n",
       "75%       0.143402     0.698887     0.401905     2.766825      51.780500   \n",
       "max       7.138400   441.740000     0.996880   782.250000   70686.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  9496.000000  9496.000000  9496.000000  9496.000000  9496.000000  ...   \n",
       "mean     -0.046815     0.143469     3.802624     2.910833     0.400218  ...   \n",
       "std       4.275350     6.697446    28.988943   100.010548     4.605299  ...   \n",
       "min    -398.120000   -61.628000    -1.594500    -0.000857  -440.740000  ...   \n",
       "25%       0.000000     0.002899     0.409535     1.029000     0.283410  ...   \n",
       "50%       0.000000     0.062915     1.028300     1.295700     0.495375  ...   \n",
       "75%       0.080294     0.168065     2.542125     2.262225     0.704827  ...   \n",
       "max      14.756000   649.230000  1597.400000  9742.300000     0.999370  ...   \n",
       "\n",
       "                45           47           49           50            54  \\\n",
       "count  9496.000000  9496.000000  9496.000000  9496.000000  9.496000e+03   \n",
       "mean      2.586799     0.088178     2.744910     0.467164  6.174652e+03   \n",
       "std      14.118622     6.412551    12.526885     4.602961  6.736910e+04   \n",
       "min      -5.256100    -9.679200     0.002264     0.000516 -1.805200e+06   \n",
       "25%       0.592220    -0.038386     0.759858     0.194315  1.466975e+01   \n",
       "50%       1.015050     0.024399     1.206650     0.348455  8.793950e+02   \n",
       "75%       1.896000     0.123900     2.172925     0.544880  4.181975e+03   \n",
       "max     782.250000   623.850000   544.560000   441.740000  3.657400e+06   \n",
       "\n",
       "                56            57           58            60            62  \n",
       "count  9496.000000   9496.000000  9496.000000   9496.000000   9496.000000  \n",
       "mean     -0.035702     13.234083     0.703249     16.855230     11.426099  \n",
       "std      12.606009    653.658670    29.089834    305.222767    241.689277  \n",
       "min    -979.250000     -4.549700  -189.580000     -0.007521     -0.367890  \n",
       "25%       0.011969      0.871975     0.000000      4.725850      3.149050  \n",
       "50%       0.129680      0.948945     0.005109      7.005200      5.273050  \n",
       "75%       0.312032      0.993080     0.247670     11.079250      9.097200  \n",
       "max     147.190000  59672.000000  2517.700000  26862.000000  23454.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5f1W5c5fs-bj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>47</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>54</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9.496000e+03</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9.496000e+03</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "      <td>9496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.484545</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.998244</td>\n",
       "      <td>7.971612e-03</td>\n",
       "      <td>0.863980</td>\n",
       "      <td>0.998392</td>\n",
       "      <td>0.490458</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>0.127988</td>\n",
       "      <td>0.998756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006133</td>\n",
       "      <td>0.904593</td>\n",
       "      <td>6.362136e-03</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.333511</td>\n",
       "      <td>0.667143</td>\n",
       "      <td>0.026899</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.030385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.048437</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>3.465338e-02</td>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.472060</td>\n",
       "      <td>0.043592</td>\n",
       "      <td>4.397421</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033475</td>\n",
       "      <td>0.727834</td>\n",
       "      <td>2.905675e-02</td>\n",
       "      <td>0.009570</td>\n",
       "      <td>0.084494</td>\n",
       "      <td>0.109297</td>\n",
       "      <td>1.328575</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.230444</td>\n",
       "      <td>0.642722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.863417</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>4.281891e-07</td>\n",
       "      <td>-1.728136</td>\n",
       "      <td>0.216602</td>\n",
       "      <td>-3.863417</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.081450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012462</td>\n",
       "      <td>-0.204019</td>\n",
       "      <td>4.281891e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.938318</td>\n",
       "      <td>-7.822884</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>-0.004572</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.480409</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>2.405887e-03</td>\n",
       "      <td>0.864370</td>\n",
       "      <td>0.998484</td>\n",
       "      <td>0.480550</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.045245</td>\n",
       "      <td>0.998513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.890227</td>\n",
       "      <td>1.757705e-03</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.325785</td>\n",
       "      <td>0.667557</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.008374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.483995</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>3.571461e-03</td>\n",
       "      <td>0.864654</td>\n",
       "      <td>0.998484</td>\n",
       "      <td>0.484780</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.056971</td>\n",
       "      <td>0.998954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.897354</td>\n",
       "      <td>2.794063e-03</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.326869</td>\n",
       "      <td>0.668577</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.014023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.490453</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.998807</td>\n",
       "      <td>6.412969e-03</td>\n",
       "      <td>0.864966</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>0.492191</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.099469</td>\n",
       "      <td>0.999389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.908647</td>\n",
       "      <td>5.035388e-03</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.331011</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.024192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.983486</td>\n",
       "      <td>0.918455</td>\n",
       "      <td>1.000044</td>\n",
       "      <td>1.814464e+00</td>\n",
       "      <td>1.282856</td>\n",
       "      <td>1.027464</td>\n",
       "      <td>46.240405</td>\n",
       "      <td>2.403578</td>\n",
       "      <td>428.364772</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.854684</td>\n",
       "      <td>71.702465</td>\n",
       "      <td>1.263130e+00</td>\n",
       "      <td>0.918455</td>\n",
       "      <td>4.912883</td>\n",
       "      <td>1.943626</td>\n",
       "      <td>121.284553</td>\n",
       "      <td>0.108546</td>\n",
       "      <td>20.280861</td>\n",
       "      <td>62.371024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2             3            4   \\\n",
       "count  9496.000000  9496.000000  9496.000000  9.496000e+03  9496.000000   \n",
       "mean      0.484545     0.001209     0.998244  7.971612e-03     0.863980   \n",
       "std       0.048437     0.009576     0.009565  3.465338e-02     0.040973   \n",
       "min      -3.863417    -0.000002     0.081549  4.281891e-07    -1.728136   \n",
       "25%       0.480409     0.000573     0.998004  2.405887e-03     0.864370   \n",
       "50%       0.483995     0.001001     0.998374  3.571461e-03     0.864654   \n",
       "75%       0.490453     0.001450     0.998807  6.412969e-03     0.864966   \n",
       "max       0.983486     0.918455     1.000044  1.814464e+00     1.282856   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  9496.000000  9496.000000  9496.000000  9496.000000  9496.000000  ...   \n",
       "mean      0.998392     0.490458     0.007219     0.127988     0.998756  ...   \n",
       "std       0.008397     0.472060     0.043592     4.397421     0.009576  ...   \n",
       "min       0.216602    -3.863417    -0.000897    -0.000038     0.081450  ...   \n",
       "25%       0.998484     0.480550     0.002116     0.045245     0.998513  ...   \n",
       "50%       0.998484     0.484780     0.003047     0.056971     0.998954  ...   \n",
       "75%       0.998641     0.492191     0.005323     0.099469     0.999389  ...   \n",
       "max       1.027464    46.240405     2.403578   428.364772     1.000002  ...   \n",
       "\n",
       "                45           47            49           50           54  \\\n",
       "count  9496.000000  9496.000000  9.496000e+03  9496.000000  9496.000000   \n",
       "mean      0.006133     0.904593  6.362136e-03     0.000968     0.333511   \n",
       "std       0.033475     0.727834  2.905675e-02     0.009570     0.084494   \n",
       "min      -0.012462    -0.204019  4.281891e-07    -0.000002    -1.938318   \n",
       "25%       0.001404     0.890227  1.757705e-03     0.000401     0.325785   \n",
       "50%       0.002406     0.897354  2.794063e-03     0.000721     0.326869   \n",
       "75%       0.004495     0.908647  5.035388e-03     0.001130     0.331011   \n",
       "max       1.854684    71.702465  1.263130e+00     0.918455     4.912883   \n",
       "\n",
       "                56           57           58           60           62  \n",
       "count  9496.000000  9496.000000  9496.000000  9496.000000  9496.000000  \n",
       "mean      0.667143     0.026899     0.003378     0.012726     0.030385  \n",
       "std       0.109297     1.328575     0.001215     0.230444     0.642722  \n",
       "min      -7.822884    -0.009247    -0.004572    -0.000006    -0.000978  \n",
       "25%       0.667557     0.001772     0.003349     0.003568     0.008374  \n",
       "50%       0.668577     0.001929     0.003349     0.005289     0.014023  \n",
       "75%       0.670158     0.002018     0.003359     0.008365     0.024192  \n",
       "max       1.943626   121.284553     0.108546    20.280861    62.371024  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tupWwybys-bk",
    "tags": []
   },
   "source": [
    "## 2. Optimisation and learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "Bankruptcy probability prediction\n",
    "\n",
    "For each instance *i*, the model has to return the estimated probability $\\hat{p}^{(i)}$ that the firm will go bankrupt conditional on the corresponding vector of features $x^{(i)}$.\n",
    "\n",
    "The prediction wil be made using the logistic function. With the logistic function, conditional probabilities are computed as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\hat{p}\\left( y^{(i)} = 1 | \\omega, b, x^{(i)} \\right) & = \\sigma\\left(\\omega^{T} x^i + b\\right) \\\\\n",
    "        \\hat{p}\\left( y^{(i)} = 0 | \\omega, b, x^{(i)} \\right) & = 1 - \\sigma\\left(\\omega^{T} x^i + b\\right)\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "where:\n",
    "\n",
    "- $x^{(i)}$ is a column vector with 37 components (number of remaining features);\n",
    "- $w$ is a column vector of $n=37$ weights (coefficients);\n",
    "- $b$ is a bias term (intercept/constant);\n",
    "- $\\sigma$ is the logistic function, i.e. $\\sigma(t) = \\frac{1}{\\left(1+\\exp(-t)\\right)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "The prediction rule is as follows:\n",
    "\\begin{equation}\n",
    "    \\hat{y}^{(i)} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        0 & \\mbox{ if } \\hat{p}^{(i)} < 0.5 \\\\\n",
    "        1 & \\mbox{ if } \\hat{p}^{(i)} \\geq 0.5\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "Replacing $\\hat{p}^{(i)}$ by its expression as computed using the logistic function we get the following:\n",
    "<img src=\"Logistic-Function.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "### 2.2 Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model aims at determining the $\\omega$ and $b$ values such that the model estimates a high $\\hat{p}^{(i)}$ value when $y^{(i)}=1$ and a low $\\hat{p}^{(i)}$ value when $y^{(i)}=0$\n",
    "\n",
    "This is achieved by defining a cost function $F(\\omega,b)$ which, for example $i$, is computed as:\n",
    "\\begin{equation}\n",
    "    F^{(i)}(\\omega,b) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        -log(\\hat{p}^{(i)}) & \\mbox{ if } y^{(i)} = 1 \\\\\n",
    "        -log(1 - \\hat{p}^{(i)}) & \\mbox{ if } y^{(i)} = 0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "For a set of $m$ instances (or a batch of size $m$), the cost expresses:\n",
    "$$ F(\\omega,b) = - \\frac{1}{m}\\sum^{m}_{i=1} y^{(i)} \\log \\left[ \\sigma \\left(\\omega^T x^{(i)} + b \\right) \\right]+ (1 - y^{(i)}) \\log \\left[ 1 - \\sigma \\left( \\omega^T x^{(i)} + b \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the total cost increases with errors:\n",
    "\n",
    "- if the model predicts a low $\\hat{p}^{(i)}$ probability of bankruptcy for fimr $i$ whereas it goes bankrupt, i.e. when $y^{(i)} = 1$, $- y^{(i)}\\log\\left(\\hat{p}^{(i)} \\right)$ is large;\n",
    "- if the model predicts a large $\\hat{p}^{(i)}$ probability of bankruptcy for fimr $i$ whereas it survives, i.e. when $y^{(i)} = 0$, $- (1 - y^{(i)})\\log \\left(1 - \\hat{p}^{(i)} \\right)$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the cost function is the opposite of the log-likelihood:\n",
    "\n",
    "- Assuming indepedence observations, the likelihood of observing the sample is:\n",
    "\\begin{eqnarray*}\n",
    "    L(\\omega,b) & = & \\prod^{m}_{i=1} Pr\\left( Y = y^{(i)} | X = x^{(i)}\\right) \\\\\n",
    "    & = & \\prod^{m}_{i=1} \\sigma\\left(\\omega^T x^{(i)} + b \\right)^{y^{(i)}} . \\left(1 - \\sigma\\left(\\omega^T x^{(i)} + b \\right)\\right)^{(1-y^{(i)})}  \n",
    "\\end{eqnarray*}\n",
    "    \n",
    "- Therefore the expression for the log-likelihood is\n",
    "$$ LL(\\omega,b) = \\sum^{m}_{i=1} y^{(i)} \\log \\left[ \\sigma \\left(\\omega^T x^{(i)} + b \\right) \\right] + (1 - y^{(i)}) \\log \\left[ 1 - \\sigma \\left( \\omega^T x^{(i)} + b \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "***Def:*** The gradient of f: $\\mathbb{R}^{n} \\mapsto \\mathbb{R}$ is $$\\nabla f (x)=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n} \n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n} $$\n",
    "i.e., a vector that gathers the partial derivatives of f. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0w7wNG1cYDv"
   },
   "source": [
    "### 2.3 Compute gradient with JAX framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Hsz0G4pRcb-F"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def jnp_sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# Computes our network's output\n",
    "def reg_logistic(params, x):\n",
    "    w, b = params\n",
    "    return jnp_sigmoid(jnp.dot(w, x) + b)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cost(params, x, y):\n",
    "    out = reg_logistic(params, x)\n",
    "    cross_entropy = -y * jnp.log(out) - (1 - y)*jnp.log(1 - out)\n",
    "    return cross_entropy[0]\n",
    "\n",
    "# compute loss gradient\n",
    "loss_grad = grad(cost)\n",
    "\n",
    "def prediction(params,x):\n",
    "    w, b = params\n",
    "    out = jnp_sigmoid(np.dot(w,x)+b)\n",
    "    if out >= .5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "#### 2.3.2 Algorithm to find the minimal cost : Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "<img src=\"Gradient_Descent.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLlxgI2Ss-bg",
    "tags": []
   },
   "source": [
    "##### 2.3.2.1 Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In batch gradient descent, parameters are updated using the full training set at every step\n",
    "\n",
    "Gradient descent algorithm:\n",
    "1. Initialize $\\omega$ components and $b$ with random values\n",
    "2. Iterate until convergence to minimum cost (or other stopping criterion):\n",
    "    1. For all $j \\in [1,n]: \\omega_j := w_j - \\eta \\frac{\\partial}{\\partial \\omega_j} J(\\omega,b)$\n",
    "    2. $b := b -\\eta \\frac{\\partial}{\\partial b} J(\\omega,b)$\n",
    "where $\\eta$ is call the learning rate. It determines the size of the downhill step\n",
    "\n",
    "As $\\eta$ is at the discretion of the modeler, it is a hyperparameter of the model:\n",
    "- If it is too small, convergence will be slow and potentiall will not be reached;\n",
    "- If it is too high, the parameter values may bounce around their 'true' value and potentially the algorithm may diverge.\n",
    "\n",
    "<img src=\"Batch_Gradient_Descent.png\" width=70%>\n",
    "\n",
    "The number of iterations is another parameter that is at the discretion of the modeler\n",
    "- If the number of iterations is too low, the model will not have enough steps to reach the minimul cost;\n",
    "- If it is too high, there will ne unneccessary steps when the gradient value is near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "773sjwqWs-bl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss epoch 0 : 0.6947355\n",
      "loss epoch 10 : 0.6913588\n",
      "loss epoch 20 : 0.68982583\n",
      "loss epoch 30 : 0.7279116\n",
      "loss epoch 40 : 0.70077795\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "loss_list=[]\n",
    "np.random.seed(42)\n",
    "# random initialization of w and b\n",
    "w = np.random.uniform(low=-1, high=1, size=Xtrain_norm.shape[1]) \n",
    "b = np.random.uniform(low=-1, high=1, size=1)\n",
    "params=[w,b]\n",
    "for epoch in range(100):\n",
    "    # Shuffle randomly Xtrain_norm\n",
    "    Xtrain_rand = Xtrain_norm.sample(frac=1)\n",
    "    # Reorder Ytrain in the same order as Xtrain_rand\n",
    "    Ytrain_rand = Ytrain.reindex(Xtrain_rand.index)\n",
    "    for i in range(Xtrain_rand.shape[0]):\n",
    "        x = Xtrain_rand.iloc[i,:]\n",
    "        y = Ytrain_rand.iloc[i]\n",
    "        grad_w = loss_grad(params, jnp.array(x), jnp.array(y))[0]\n",
    "        grad_b =  loss_grad(params, jnp.array(x), jnp.array(y))[1]\n",
    "        w -=eta * grad_w\n",
    "        b -=eta * grad_b\n",
    "        params=[w,b]\n",
    "    loss = 0\n",
    "    for i in range(Xtrain_rand.shape[0]):\n",
    "        x = Xtrain_rand.iloc[i,:]\n",
    "        y = Ytrain_rand.iloc[i]\n",
    "        loss += cost(params,jnp.array(x),jnp.array(y))\n",
    "    loss = loss / Xtrain_rand.shape[0]\n",
    "    loss_list.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"loss epoch\", epoch, \":\", loss)\n",
    "print(\"loss epoch\", epoch, \":\", loss)\n",
    "print(\"\\nfinished\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXUmDppps-bl"
   },
   "source": [
    "Evolution of loss (cost) through epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEWPvWDes-bl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = np.arange(len(loss_list))\n",
    "plt.plot(epoch, loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**\n",
    "\n",
    "Accuracy (on the train set) corresponds to the ratio of correct predictions divided by the number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVcJ4Ufzs-bl"
   },
   "source": [
    "**Task 10**: Code the computation of the accuracy on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wldnZkuRs-bl"
   },
   "outputs": [],
   "source": [
    "count_ok = 0\n",
    "for i in range(Xtrain_norm.shape[0]):\n",
    "        x = Xtrain_norm.iloc[i,:]\n",
    "        y = Ytrain.iloc[i]\n",
    "        params=[w,b]\n",
    "        if prediction(params,x) == y:\n",
    "            count_ok += 1\n",
    "print(\"accuracy:{:.2f}%\".format(100 * count_ok/Xtrain_norm.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgJta610s-bm"
   },
   "source": [
    "Weights ($\\omega$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "5kqwKDrXs-bm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "w=pd.DataFrame(w)\n",
    "w.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "5nUB939xs-bn"
   },
   "outputs": [],
   "source": [
    "# vectorized version of prediction accuracy\n",
    "def accuracy(X, Y, w, b):\n",
    "    pred = (jnp_sigmoid(np.dot(X, w)+b)>=0.5)\n",
    "    check = (pred==Y)\n",
    "    return check.sum()/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "rIx-_r1us-bo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.36%\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: {:.2f}%\".format(100 * accuracy(Xtrain_norm, Ytrain, w, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPZgjR22s-bo",
    "tags": []
   },
   "source": [
    "## 3. How good is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "KRisxG9Ls-bo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:93.24%\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "print(\"accuracy:{:.2f}%\".format(100 * accuracy(Xtest_norm, Ytest, w, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvvteT6rs-bo",
    "tags": []
   },
   "source": [
    "## 4. Save train and test non-normalized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "aDNF_ic4s-bp"
   },
   "outputs": [],
   "source": [
    "trainset = pd.concat([Xtrain, Ytrain], axis = 1)\n",
    "testset = pd.concat([Xtest, Ytest], axis = 1)\n",
    "trainset.to_csv(\"trainset_2.csv\", header=True, index=False)\n",
    "testset.to_csv(\"testset_2.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bankruptcy - Part 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
